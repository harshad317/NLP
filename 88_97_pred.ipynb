{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "88.97 pred.ipynb",
      "provenance": [],
      "mount_file_id": "1mmXf2SLHfojzrzrfBntaDkW5sDBXWDBC",
      "authorship_tag": "ABX9TyNJZo04Ow1gQH3tvyjSFqp9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshad317/NLP/blob/master/88_97_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9hpz2HWhfi7",
        "outputId": "b022cd4b-73f4-42bb-a3c8-ec6613dc271e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!wget \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-16 18:07:18--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.10.109\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.10.109|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz.1’\n",
            "\n",
            "1                    25%[====>               ] 407.89M  34.7MB/s    eta 35s    ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G78exchYjloB",
        "outputId": "563b66b5-01de-4bf5-a4fb-493c98aa7087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n",
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.2.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 289, in call\n",
            "    return p.wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 1477, in wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 1424, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/__main__.py\", line 33, in <module>\n",
            "    plac.call(commands[command], sys.argv[1:])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/plac_core.py\", line 367, in call\n",
            "    cmd, result = parser.consume(arglist)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/plac_core.py\", line 232, in consume\n",
            "    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/cli/download.py\", line 48, in download\n",
            "    dl = download_model(dl_tpl.format(m=model_name, v=version), pip_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/spacy/cli/download.py\", line 135, in download_model\n",
            "    return subprocess.call(cmd, env=os.environ.copy())\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 292, in call\n",
            "    p.wait()\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 1477, in wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 1424, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jteNXX87hHgK",
        "outputId": "fcd99fcb-085b-43cf-87c9-ab1ae6df9b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "from __future__ import absolute_import, division\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from tqdm import tqdm\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "lc = LancasterStemmer()\n",
        "from nltk.stem import SnowballStemmer\n",
        "sb = SnowballStemmer(\"english\")\n",
        "import gc\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "import sys\n",
        "from os.path import dirname\n",
        "#sys.path.append(dirname(dirname(__file__)))\n",
        "from keras import initializers\n",
        "from keras.engine import InputSpec, Layer\n",
        "from keras import backend as K\n",
        "\n",
        "import spacy\n",
        "\n",
        "# https://github.com/bfelbo/DeepMoji/blob/master/deepmoji/attlayer.py\n",
        "class AttentionWeightedAverage(Layer):\n",
        "    \"\"\"\n",
        "    Computes a weighted average of the different channels across timesteps.\n",
        "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, return_attention=False, **kwargs):\n",
        "        self.init = initializers.get('uniform')\n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(ndim=3)]\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 initializer=self.init)\n",
        "        self.trainable_weights = [self.W]\n",
        "        super(AttentionWeightedAverage, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        # computes a probability distribution over the timesteps\n",
        "        # uses 'max trick' for numerical stability\n",
        "        # reshape is done to avoid issue with Tensorflow\n",
        "        # and 1-dimensional weights\n",
        "        logits = K.dot(x, self.W)\n",
        "        x_shape = K.shape(x)\n",
        "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
        "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
        "\n",
        "        # masked timesteps have zero weight\n",
        "        if mask is not None:\n",
        "            mask = K.cast(mask, K.floatx())\n",
        "            ai = ai * mask\n",
        "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
        "        weighted_input = x * K.expand_dims(att_weights)\n",
        "        result = K.sum(weighted_input, axis=1)\n",
        "        if self.return_attention:\n",
        "            return [result, att_weights]\n",
        "        return result\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return self.compute_output_shape(input_shape)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_len = input_shape[2]\n",
        "        if self.return_attention:\n",
        "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
        "        return (input_shape[0], output_len)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        if isinstance(input_mask, list):\n",
        "            return [None] * len(input_mask)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "# https://www.kaggle.com/cpmpml/spell-checker-using-word2vec\n",
        "spell_model = gensim.models.KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
        "words = spell_model.index2word\n",
        "w_rank = {}\n",
        "for i,word in enumerate(words):\n",
        "    w_rank[word] = i\n",
        "WORDS = w_rank\n",
        "# Use fast text as vocabulary\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "def P(word): \n",
        "    \"Probability of `word`.\"\n",
        "    # use inverse of rank as proxy\n",
        "    # returns 0 if the word isn't in the dictionary\n",
        "    return - WORDS.get(word, 0)\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or [word])\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "def singlify(word):\n",
        "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\n",
        "\n",
        "# modified version of \n",
        "# https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings\n",
        "# https://www.kaggle.com/danofer/different-embeddings-with-attention-fork\n",
        "# https://www.kaggle.com/shujian/different-embeddings-with-attention-fork-fork\n",
        "def load_glove(word_dict, lemma_dict):\n",
        "    EMBEDDING_FILE = '/content/drive/My Drive/w2v/glove.6B.300d.txt'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
        "    embed_size = 300\n",
        "    nb_words = len(word_dict)+1\n",
        "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
        "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n",
        "    print(unknown_vector[:5])\n",
        "    for key in tqdm(word_dict):\n",
        "        word = key\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = key.lower()\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = key.upper()\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = key.capitalize()\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = ps.stem(key)\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = lc.stem(key)\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = sb.stem(key)\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = lemma_dict[key]\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        if len(key) > 1:\n",
        "            word = correction(key)\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[word_dict[key]] = embedding_vector\n",
        "                continue\n",
        "        embedding_matrix[word_dict[key]] = unknown_vector                    \n",
        "    return embedding_matrix, nb_words \n",
        "\n",
        "def load_fasttext(word_dict, lemma_dict):\n",
        "    EMBEDDING_FILE = '/content/drive/My Drive/w2v/crawl-300d-2M.vec'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
        "    embed_size = 300\n",
        "    nb_words = len(word_dict)+1\n",
        "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
        "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n",
        "    print(unknown_vector[:5])\n",
        "    for key in tqdm(word_dict):\n",
        "        word = key\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = key.lower()\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = key.upper()\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = key.capitalize()\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = ps.stem(key)\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = lc.stem(key)\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = sb.stem(key)\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        word = lemma_dict[key]\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[word_dict[key]] = embedding_vector\n",
        "            continue\n",
        "        if len(key) > 1:\n",
        "            word = correction(key)\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[word_dict[key]] = embedding_vector\n",
        "                continue\n",
        "        embedding_matrix[word_dict[key]] = unknown_vector                    \n",
        "    return embedding_matrix, nb_words \n",
        "\n",
        "def build_model(embedding_matrix, nb_words, embedding_size=300):\n",
        "    inp = Input(shape=(max_length,))\n",
        "    x = Embedding(nb_words, embedding_size, weights=[embedding_matrix], trainable=False)(inp)\n",
        "    x = SpatialDropout1D(0.3)(x)\n",
        "    x1 = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
        "    x2 = Bidirectional(GRU(128, return_sequences=True))(x1)\n",
        "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
        "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
        "    conc = Concatenate()([max_pool1, max_pool2])\n",
        "    predictions = Dense(1, activation='sigmoid')(conc)\n",
        "    model = Model(inputs=inp, outputs=predictions)\n",
        "    adam = optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"Loading data ...\")\n",
        "train = pd.read_csv('/content/train_2kmZucJ.csv').fillna(' ')\n",
        "test = pd.read_csv('/content/test_oJQbWVk.csv').fillna(' ')\n",
        "train_text = train['tweet']\n",
        "test_text = test['tweet']\n",
        "text_list = pd.concat([train_text, test_text])\n",
        "y = train['label'].values\n",
        "num_train_data = y.shape[0]\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"Spacy NLP ...\")\n",
        "nlp = spacy.load('en_core_web_lg', disable=['parser','ner','tagger'])\n",
        "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
        "word_dict = {}\n",
        "word_index = 1\n",
        "lemma_dict = {}\n",
        "docs = nlp.pipe(text_list, n_threads = 2)\n",
        "word_sequences = []\n",
        "for doc in tqdm(docs):\n",
        "    word_seq = []\n",
        "    for token in doc:\n",
        "        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n",
        "            word_dict[token.text] = word_index\n",
        "            word_index += 1\n",
        "            lemma_dict[token.text] = token.lemma_\n",
        "        if token.pos_ is not \"PUNCT\":\n",
        "            word_seq.append(word_dict[token.text])\n",
        "    word_sequences.append(word_seq)\n",
        "del docs\n",
        "gc.collect()\n",
        "train_word_sequences = word_sequences[:num_train_data]\n",
        "test_word_sequences = word_sequences[num_train_data:]\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "# hyperparameters\n",
        "max_length = 55\n",
        "embedding_size = 600\n",
        "learning_rate = 0.001\n",
        "batch_size = 512\n",
        "num_epoch = 5\n",
        "\n",
        "train_word_sequences = pad_sequences(train_word_sequences, maxlen=max_length, padding='post')\n",
        "test_word_sequences = pad_sequences(test_word_sequences, maxlen=max_length, padding='post')\n",
        "print(train_word_sequences[:1])\n",
        "print(test_word_sequences[:1])\n",
        "pred_prob = np.zeros((len(test_word_sequences),), dtype=np.float32)\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"Loading embedding matrix ...\")\n",
        "embedding_matrix_glove, nb_words = load_glove(word_dict, lemma_dict)\n",
        "embedding_matrix_fasttext, nb_words = load_fasttext(word_dict, lemma_dict)\n",
        "embedding_matrix = np.concatenate((embedding_matrix_glove, embedding_matrix_fasttext), axis=1)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"Start training ...\")\n",
        "model = build_model(embedding_matrix, nb_words, embedding_size)\n",
        "model.fit(train_word_sequences, y, batch_size=batch_size, epochs=num_epoch-1, verbose=2)\n",
        "pred_prob += 0.15*np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2))\n",
        "model.fit(train_word_sequences, y, batch_size=batch_size, epochs=1, verbose=2)\n",
        "pred_prob += 0.35*np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2))\n",
        "del model, embedding_matrix_fasttext\n",
        "gc.collect()\n",
        "K.clear_session()\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "\n",
        "\n",
        "#submission = pd.DataFrame.from_dict({'qid': test['qid']})\n",
        "#submission['prediction'] = (pred_prob>0.35).astype(int)\n",
        "#submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "--- 0.03155970573425293 seconds ---\n",
            "Spacy NLP ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9873it [00:03, 3290.74it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- 34.17848873138428 seconds ---\n",
            "[[ 1  2  1  3  4  5  1  6  1  7  1  8  1  9  1 10  1 11  1 12  1 13  1 14\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0]]\n",
            "[[   50  1150   184   157     1    14  2355   172 20387    68   506    70\n",
            "   1149     7   172     1  5259     1   154   725     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0]]\n",
            "Loading embedding matrix ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 893/31320 [00:00<00:03, 8911.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[-1. -1. -1. -1. -1.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31320/31320 [00:09<00:00, 3242.73it/s]\n",
            "  3%|▎         | 850/31320 [00:00<00:03, 8384.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[-1. -1. -1. -1. -1.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31320/31320 [00:09<00:00, 3460.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- 178.35113167762756 seconds ---\n",
            "Start training ...\n",
            "Epoch 1/4\n",
            "16/16 - 143s - loss: 0.4119 - accuracy: 0.7848\n",
            "Epoch 2/4\n",
            "16/16 - 145s - loss: 0.2933 - accuracy: 0.8741\n",
            "Epoch 3/4\n",
            "16/16 - 148s - loss: 0.2502 - accuracy: 0.8926\n",
            "Epoch 4/4\n",
            "16/16 - 145s - loss: 0.2358 - accuracy: 0.9027\n",
            "4/4 - 11s\n",
            "16/16 - 143s - loss: 0.2165 - accuracy: 0.9086\n",
            "4/4 - 11s\n",
            "--- 813.5954883098602 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1duzSBXsiCM0",
        "outputId": "a101f978-74db-42b5-b13c-7c29c443dfe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "start_time = time.time()\n",
        "print(\"Start training ...\")\n",
        "model = build_model(embedding_matrix, nb_words, embedding_size)\n",
        "model.fit(train_word_sequences, y, batch_size=batch_size, epochs=num_epoch-1, verbose=2)\n",
        "pred_prob += 0.15*np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2))\n",
        "model.fit(train_word_sequences, y, batch_size=batch_size, epochs=1, verbose=2)\n",
        "pred_prob += 0.35*np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2))\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training ...\n",
            "Epoch 1/4\n",
            "16/16 - 144s - loss: 0.4151 - accuracy: 0.7840\n",
            "Epoch 2/4\n",
            "16/16 - 146s - loss: 0.2894 - accuracy: 0.8750\n",
            "Epoch 3/4\n",
            "16/16 - 142s - loss: 0.2537 - accuracy: 0.8922\n",
            "Epoch 4/4\n",
            "16/16 - 147s - loss: 0.2301 - accuracy: 0.9042\n",
            "4/4 - 11s\n",
            "16/16 - 147s - loss: 0.2155 - accuracy: 0.9115\n",
            "4/4 - 11s\n",
            "--- 812.2090911865234 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDh8Zen732NP"
      },
      "source": [
        "submission = pd.DataFrame()\n",
        "submission['id'] = test['id']\n",
        "submission['label'] = (pred_prob>0.35).astype(int)\n",
        "submission.to_csv('submission_big.csv', index= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSg4jwOO5QI6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}